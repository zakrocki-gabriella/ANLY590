{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbORImzGwDO1"
   },
   "source": [
    "#### Building a model from a pretrained GloVe model\n",
    "- Training your own model embedding model is possible however it frequently requires a lot of data\n",
    "- What we more frequently do is start with an embedding model trained by Facebook, Google or Stanford and then either:\n",
    "    1. Freeze the embedding layer - assuming the linguistic regularity of our corpus/documents matches that off the embeddings generated by the pretrained models\n",
    "    2. Use the pretrained weights and update them for our model\n",
    "\n",
    "- In this class we won't go over fitting skipgram, word2vec or GloVe models from scratch because they require a lot of data and computation resources\n",
    "- For a more thorough exposition see the following links:\n",
    "    - [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "    - [Python gensim Word2Vec tutorial with TensorFlow and Keras](http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/)\n",
    "    - [A Word2Vec Keras tutorial](http://adventuresinmachinelearning.com/word2vec-keras-tutorial/)\n",
    "    - [Code example: Word2Vec (skipgram) in Keras with Gensim](https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnfd_Q5pwDO4"
   },
   "source": [
    "#### What we are going to be doing in this notebook\n",
    "- In this notebook we'll go through using pretrained embeddings on our imdb data set\n",
    "- When there isn't sufficient data, using some form of pre-trained mode is really beneficial (transfer learning)\n",
    "\n",
    "- In sequence we will:\n",
    "    - Download the raw imdb data set, unzip it, read it and assign pos/neg target values\n",
    "    - Tokenize the imdb data set\n",
    "    - Download GloVe word embeddings\n",
    "    - Prepare the Glove word-embedding matrix\n",
    "    - Setup our neural netowrk \n",
    "    - Load GloVe embedding in the model\n",
    "        - Note we will freeze this layer so that it does not disrupt pre-trained weights\n",
    "    - Training the model\n",
    "    - Compare the model without pretrained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiCrRVUbwDO5"
   },
   "source": [
    "### Setting up file imports for IMDB data\n",
    "- Here we download the raw individual comments from IMDB users from -`https://www.kaggle.com/iarunava/imdb-movie-reviews-dataset`\n",
    "- The file structure is such that you have:\n",
    "    - /aclImdb -> (\n",
    "        - /test -> \n",
    "            - (/neg,/pos, urls_neg.txt, url_post.txt), \n",
    "        - /train -> \n",
    "            - (/neg,/pos, urls_neg.txt, url_post.txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoDKJF9DwDO6"
   },
   "outputs": [],
   "source": [
    "# -- Import libraries --\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import numpy as np\n",
    "import os\n",
    "# Restrict to 10000 most common words\n",
    "max_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "9IAf8YJSwDPA",
    "outputId": "03aa8d2a-2061-4d4c-837a-966c0545d083"
   },
   "outputs": [],
   "source": [
    "# LOAD IMDB DATA\n",
    "(x_train_val, y_train_val), (x_test, y_test) = imdb.load_data(num_words = max_words)\n",
    "max_val_word_index = max([max(sequence) for sequence in x_train_val])\n",
    "max_length_review = max([len(sequence) for sequence in x_train_val])\n",
    "\n",
    "# Printing output\n",
    "print(\" Train and Validation data {x}\\n Train labels {y}\".format(x = x_train_val.shape, y = y_train_val.shape))\n",
    "print(\"_\"*20)\n",
    "print(\" Test and Validation  data {x}\\n Test labels {y}\".format(x = x_test.shape, y = y_test.shape))\n",
    "print(\"_\"*20)\n",
    "print(\"Maximum value of a word index {}\".format(max_val_word_index))\n",
    "print(\"Maximum length num words of review in train {}\".format(max_length_review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "2b_3ub-JwDPD",
    "outputId": "abf2c9b9-528c-4f03-ad8f-362f4ddf5589"
   },
   "outputs": [],
   "source": [
    "# --- Getting reviews ---\n",
    "# Reverse from integers to words using the DICTIONARY (given by keras...need to do nothing to create it)\n",
    "# note that words without a mapping are given by a \"?\"\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train_val[123]])\n",
    "\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvCIgeAWwDPI"
   },
   "outputs": [],
   "source": [
    "# -- Parameter setup --\n",
    "# Cutoff reviews after 100 words\n",
    "maxlen = 100\n",
    "# Train on 20000 samples\n",
    "training_samples = 20000\n",
    "# Validation on 5000 samples\n",
    "validation_samples = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "7EJZOEchwDPL",
    "outputId": "9ef3d747-90ae-44d5-cecd-d8d4f9eed2ca"
   },
   "outputs": [],
   "source": [
    "# Padding/bounding number of words in a sequence\n",
    "data = pad_sequences(x_train_val, maxlen = maxlen)\n",
    "labels = np.asarray(y_train_val)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ogtip5UIwDPN"
   },
   "outputs": [],
   "source": [
    "# -- Shuffling indices --\n",
    "np.random.seed(1234)\n",
    "# Shuffling data set  because ordered\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "# --- Training and validation set ---\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# - Setting up train/test split -\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples:(training_samples + validation_samples)]\n",
    "y_val = labels[training_samples:(training_samples + validation_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "21NReW9kwDPR",
    "outputId": "69c33ea2-ea8f-4af4-859c-c930a9a3662a"
   },
   "outputs": [],
   "source": [
    "# Downloading the glove file\n",
    "# WARNING - this is a big file, about 1GB.\n",
    "import urllib\n",
    "urllib.request.urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", '/tmp/glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10UEdozXSX2r"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile(\"/tmp/glove\", 'r')\n",
    "zip_ref.extractall(\"/tmp/glove.6B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpumBmVgwDPU"
   },
   "source": [
    "#### Build a vector map \n",
    "- We are going to use a 100 dimensional glove embedding\n",
    "- Before we do so we need to build a dictionary of words:vector_embeddings\n",
    "- Next we build an index that maps words (as strings) to their vector represenation in 100D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "dGhjqXc4wDPV",
    "outputId": "18830ed0-868c-4023-bcf8-018aa724e5d5"
   },
   "outputs": [],
   "source": [
    "# --- Parsing the GloVe word-embeddings file --\n",
    "# After unzipping file \n",
    "\n",
    "glove_dir = os.path.join('/tmp/', 'glove.6B')\n",
    "\n",
    "# Dictionary where we store the word:vector_embedding map\n",
    "embeddings_index = {}\n",
    "word_index = {}\n",
    "count=0\n",
    "\n",
    "# Setting up embedding array\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:\n",
    "  for line in f:\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      coefs = np.asarray(values[1:], dtype='float32')\n",
    "      # Embeddings is a dictionary of words:word_vector_embeddings\n",
    "      embeddings_index[word] = coefs\n",
    "      word_index[word] = count\n",
    "      count+=1\n",
    "\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "mOWcAbpWwDPZ",
    "outputId": "ca612088-7bf2-4f31-9c6f-dfb4173ce9cb"
   },
   "outputs": [],
   "source": [
    "# What is the embedding of the word 'happy'\n",
    "embeddings_index['happy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6jwM14QwDPb"
   },
   "source": [
    "- Here we setup the embedding matrix with only the words that we need\n",
    "- Note that we loop over the file only selecting the most frequently occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "e3E6jHoawDPc",
    "outputId": "83a41dd6-b704-40fb-972b-7921deb827b5"
   },
   "outputs": [],
   "source": [
    "print('Our word index dictionary is given by: (word, index), a sample 10 entries are:')\n",
    "list(word_index.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6bKG_i6wDPg"
   },
   "outputs": [],
   "source": [
    "# --- Preprocessing the GloVe word-embeddings matrix --\n",
    "embedding_dim = 100\n",
    "\n",
    "# Instantiating a 10000 x 100 matrix \n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    # Make sure that we are not exceeding the max token size\n",
    "    if i < max_words:\n",
    "        # Get the embedded vector for the word\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        # Provided that a word is known store it in the \n",
    "        # embeddig matrix at position i\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i, :] = embedding_vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0HaA864FwDPi",
    "outputId": "717bcaec-fa5a-43f7-f385-a2c9320e4a18"
   },
   "outputs": [],
   "source": [
    "print(\"The size of the word embedding matrix is:\" + str(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "gMiOy62TwDPm",
    "outputId": "d0e42973-f240-416e-c870-30bc75e429c6"
   },
   "outputs": [],
   "source": [
    "# --- Definining our model ---\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "# -- Using the GloVe Embedding to train our model ---\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V76cU6w6MWzp"
   },
   "source": [
    "Note that we already have over 1 million parameters in this network. Most of those are contained in the embedding layer. \n",
    "\n",
    "But those embeddings weights aren't free parameters. Instead, we'll use the pre-trained Glove embedding and insert them into the model, and then freeze those weights so that they won't be touched during gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6w4GXxYwDPr"
   },
   "source": [
    "- Since we are interested in using our pretrain model we need to :\n",
    "    1. Load the weights into the first layer\n",
    "    2. Freeze that layer to make sure that during training it does not get updated\n",
    "- Here we load the pretrained weights into the first layer (embedding layer) and then freeze the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eH4T96K4wDPs"
   },
   "outputs": [],
   "source": [
    "# Setting up the weights\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "\n",
    "# Freeze or train the GloVe layer\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "FwW7XwFkMzS3",
    "outputId": "de2fc804-f893-41fd-8456-37926efecc55"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DonEojcfM3HM"
   },
   "source": [
    "Note the big decrease in the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hg9F1nsawDPv",
    "outputId": "c4755c50-bb9d-4572-b553-c3d4e38d8dce"
   },
   "outputs": [],
   "source": [
    "# -- Training our model --\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train,y_train,\n",
    "                   epochs =50,\n",
    "                   batch_size = 256,\n",
    "                   validation_data = (x_val, y_val))             \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeTrR-sQwDP1"
   },
   "outputs": [],
   "source": [
    "#-- Plotting the results --\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ~ Plotting parameters ~\n",
    "# Pulling out :\n",
    "#   - Training: accuracy and loss\n",
    "#   - validation: accuracy and loss\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "RggM7WruwDP6",
    "outputId": "420d1aff-eecb-49e9-9f45-a337f0bf82cb"
   },
   "outputs": [],
   "source": [
    "# Plotting the data \n",
    "\n",
    "# Training + Valdiation Accuracy\n",
    "epochs = range(1,len(acc) + 1)\n",
    "plt.plot( epochs, acc, 'bo', label = 'Training Accuracy')\n",
    "plt.plot( epochs, val_acc, 'b', label = 'Validation Accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "# Training + Validation Loss\n",
    "plt.plot( epochs, loss, 'bo', label = 'Training Loss')\n",
    "plt.plot( epochs, val_loss, 'b', label = 'Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7L4I-uPGOGNz"
   },
   "source": [
    "What do you think of these loss curves? What might we changes in the model?\n",
    "\n",
    "**Exercise**: Change the model using some techniques we've learned about in this class and refit. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "pretrained_word_embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "4px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
